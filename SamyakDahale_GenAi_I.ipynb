{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0364577-81d9-49bf-b572-05a582aed121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MErging the 64 csv files \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# folder path containing my 64 CSV files\n",
    "folder_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\Property_data\"\n",
    "#  create list for all .csv\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# DtaFrame to store the merged csv files\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "#  Iterate over the files and append them to the DataFrame\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')  # Q1 Ans - Encoder problem occured , thus specific encoding specified \n",
    "    merged_data = pd.concat([merged_data, df], ignore_index=True)  \n",
    "\n",
    "#  Dataframe to output file\n",
    "output_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "merged_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"All CSV files have been merged into {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21dc76-1449-4825-ada5-a4f67d19d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning  # not working propoerly ; thus ignored for EDA Questions\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1 Convert 'activation_date' to datetime format\n",
    "df['activation_date'] = pd.to_datetime(df['activation_date'], format='%d-%m-%Y %H:%M', errors='coerce')\n",
    "\n",
    "#  Handle missing values\n",
    "df.fillna(df.median(numeric_only=True), inplace=True)  \n",
    "df.fillna(df.mode().iloc[0], inplace=True)  \n",
    "\n",
    "#  Clean columns with string values ('Flase' -> 'False', etc.)\n",
    "df['gym'] = df['gym'].replace({'Flase': 'False', 'true': 'True', 'False': 'False'})\n",
    "df['lift'] = df['lift'].replace({'Flase': 'False', 'true': 'True', 'False': 'False'})\n",
    "\n",
    "# Convert to boolean (0 or 1)\n",
    "df['gym'] = df['gym'].map({'True': 1, 'False': 0})\n",
    "df['lift'] = df['lift'].map({'True': 1, 'False': 0})\n",
    "\n",
    "#  Check for non-numeric values in 'gym' and 'lift' columns\n",
    "print(\"Non-numeric values in 'gym':\", df['gym'].unique())\n",
    "print(\"Non-numeric values in 'lift':\", df['lift'].unique())\n",
    "\n",
    "#  Convert categorical variables to numeric \n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "categorical_columns = ['furnishing', 'lease_type', 'locality', 'parking', 'building_type']  # Add more if needed\n",
    "for col in categorical_columns:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    " Create new features from 'activation_date' \n",
    "df['year'] = df['activation_date'].dt.year\n",
    "df['month'] = df['activation_date'].dt.month\n",
    "df['day'] = df['activation_date'].dt.day\n",
    "df['weekday'] = df['activation_date'].dt.weekday\n",
    "\n",
    "\n",
    "numeric_columns = ['rent', 'deposit', 'property_size', 'latitude', 'longitude', 'bathroom', 'floor', 'total_floor', 'property_age', 'swimming_pool', 'gym', 'lift']\n",
    "\n",
    "# Check for non-numeric \n",
    "for col in numeric_columns:\n",
    "    if not pd.to_numeric(df[col], errors='coerce').notna().all():\n",
    "        print(f\"Non-numeric values found in {col} column!\")\n",
    "\n",
    ". Scale/Normalize numeric features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "#  Remove duplicate rows \n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#  Ensure the data is cleaned and transformed\n",
    "print(df.head())  # Check the first few rows of the transformed dataset\n",
    "\n",
    "#  Save the cleaned and transformed dataset to a new file (optional)\n",
    "df.to_csv('64CSVfilesCleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251a0ef-ae1a-45ae-97a3-6ebd2779c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3\n",
    "# Int he Property_photo.tsv \n",
    "# to count the number of photos for each property_id ; use formula in C2 as \n",
    "# =IF(B2<>\"\", COUNTA(TEXTSPLIT(B2, \",\")), 0)\n",
    "# as Fill Down in all rows ; to get number of photos for each id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad484d-a3ca-48a5-8663-2410b4b833f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont the no, of ints for each propoerty id \n",
    "import pandas as pd\n",
    "\n",
    "# Load the property_interactions dataset\n",
    "file_path_property_interactions = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\property_interactions.csv\"\n",
    "df = pd.read_csv(file_path_property_interactions, header=None, names=['property_id', 'request_date'])\n",
    "\n",
    "# Count the number of interactions for each unique property_id\n",
    "interaction_count = df.groupby('property_id').size().reset_index(name='total_interactions')\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "interaction_count.to_csv('property_interactions_count.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93905741-eb76-478d-b38b-31364c03c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv has 14532 rows and 24 columns.\n"
     ]
    }
   ],
   "source": [
    "# Q6 - Counting number of rows and column\n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "# Get the shape of the DataFrame\n",
    "rows, columns = df.shape\n",
    "print(f\"The file {file_path} has {rows} rows and {columns} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe03025-0959-48d9-9f35-c3d1388daeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in 'HBR Layout': 170\n",
      "Percentage of entries in 'HBR Layout': 1.17%\n"
     ]
    }
   ],
   "source": [
    "#Q7 % of datapoints in HBR (HSR Layout) \n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "# data set has HBR Layout instead of HSR ;\n",
    "# Total number of entries \n",
    "total_entries = len(df)\n",
    "\n",
    "# Number of entries where 'locality' is 'HBR Layout'\n",
    "hbr_layout_entries = len(df[df['locality'] == 'HBR Layout'])\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (hbr_layout_entries / total_entries) * 100\n",
    "\n",
    "print(f\"Entries in 'HBR Layout': {hbr_layout_entries}\")\n",
    "print(f\"Percentage of entries in 'HBR Layout': {percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b618e79-676a-4ba8-91d0-18b6521af4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique entries in 'locality' column: 62\n"
     ]
    }
   ],
   "source": [
    "#Q8 No. of unique localities \n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#unique entries in the 'locality' column\n",
    "uniqueL = df['locality'].nunique()\n",
    "\n",
    "print(f\"Number of unique entries in 'locality' column: {uniqueL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74e44217-f463-478d-975c-53973a1b252a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The locality with the highest average rent is: Bellandur\n",
      "The highest average rent is: 25949.46325878594\n"
     ]
    }
   ],
   "source": [
    "#Q9 - \n",
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Group the data by 'locality' and calculate the mean rent for each locality\n",
    "average_rent_by_locality = df.groupby('locality')['rent'].mean()\n",
    "\n",
    "# Find the locality with the highest average rent\n",
    "locality_with_highest_rent = average_rent_by_locality.idxmax()\n",
    "\n",
    "# Print\n",
    "highest_rent = average_rent_by_locality.max()\n",
    "print(f\"The locality with the highest average rent is: {locality_with_highest_rent}\")\n",
    "print(f\"The highest average rent is: {highest_rent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "902c84a4-4ecf-4caa-b1e5-18c96c774dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature with the highest correlation with rent is: bathroom\n",
      "The correlation value is: 0.6327501496590155\n"
     ]
    }
   ],
   "source": [
    "# Q10 -  \n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure column names are standardized\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "numeric_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calculate the correlation of all numeric features with 'rent'\n",
    "correlation_with_rent = numeric_features.corr()['rent'].drop('rent')\n",
    "\n",
    "# Find  highest correlation with 'rent'\n",
    "highest_correlation_feature = correlation_with_rent.idxmax()\n",
    "highest_correlation_value = correlation_with_rent.max()\n",
    "\n",
    "print(f\"The feature with the highest correlation with rent is: {highest_correlation_feature}\")\n",
    "print(f\"The correlation value is: {highest_correlation_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d0c30d3-3a8b-431f-afe3-05f665e0002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property age category added. The updated file is saved at: C:\\\\Users\\\\Samyak\\\\Downloads\\\\updated_file.csv\n",
      "The most occurring entry in 'property_age_category' is 'New'.\n"
     ]
    }
   ],
   "source": [
    "#Q12 \n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "def Age_cat(age):\n",
    "    if age <= 1:\n",
    "        return 'New'\n",
    "    elif age <= 5:\n",
    "        return 'Less than 5 years'\n",
    "    elif age <= 10:\n",
    "        return '5 to 10 years'\n",
    "    elif age <= 20:\n",
    "        return '10 to 20 years'\n",
    "    else:\n",
    "        return 'More than 20 years'\n",
    "\n",
    "# categorization function applying \n",
    "df['property_age_category'] = df['property_age'].apply(Age_cat)\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "output_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\updated_file.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Property age category added. The updated file is saved at: {output_path}\")\n",
    "\n",
    "df = pd.read_csv(output_path)\n",
    "most_common_category = df['property_age_category'].mode()[0]\n",
    "print(f\"The most occurring entry in 'property_age_category' is '{most_common_category}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfbe0ae6-d542-41f3-a8b8-e3bbec4ed865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apartment type with the highest interactions: BHK2\n"
     ]
    }
   ],
   "source": [
    "#Q13 - \n",
    "type_intractn = df['type'].value_counts()\n",
    "highest_intractn_type = type_intractn.idxmax()\n",
    "print(f\"Apartment type with the highest interactions: {highest_intractn_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd1ffe30-fde7-495f-b8d2-0e88e23b2c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdaada70-2ecc-4663-be1e-89a9f55ec198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The locality with the highest average rent is: Bellandur\n",
      "The highest average rent is: 25949.46325878594\n"
     ]
    }
   ],
   "source": [
    "#Q9 - \n",
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Group by 'locality' && calculate the mean rent for each locality\n",
    "average_rent_by_locality = df.groupby('locality')['rent'].mean()\n",
    "\n",
    "# Find the locality with the highest average rent\n",
    "locality_with_highest_rent = average_rent_by_locality.idxmax()\n",
    "\n",
    "# Print result\n",
    "highest_rent = average_rent_by_locality.max()\n",
    "print(f\"The locality with the highest average rent is: {locality_with_highest_rent}\")\n",
    "print(f\"The highest average rent is: {highest_rent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0331e17b-cfb5-4902-89ec-da85d222d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent time category is: Afternoon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samyak\\AppData\\Local\\Temp\\ipykernel_12808\\3976876600.py:4: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['activation_date'] = pd.to_datetime(df['activation_date'])\n"
     ]
    }
   ],
   "source": [
    "# Q18 - \n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df['activation_date'] = pd.to_datetime(df['activation_date'])\n",
    "\n",
    "df['hour'] = df['activation_date'].dt.hour\n",
    "\n",
    "df['time_category'] = pd.cut(df['hour'],\n",
    "                             bins=[-1, 5, 11, 17, 23],\n",
    "                             labels=['Midnight', 'Morning', 'Afternoon', 'Evening'],\n",
    "                             right=True)\n",
    "\n",
    "most_frequent_time_category = df['time_category'].mode()[0]\n",
    "\n",
    "print(f\"The most frequent time category is: {most_frequent_time_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8bd216e-0b15-42a6-b2d2-15c7f17aebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most properties were activated on the following date(s): [datetime.date(2017, 3, 17)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samyak\\AppData\\Local\\Temp\\ipykernel_12808\\1837633788.py:9: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['activation_date'] = pd.to_datetime(df['activation_date'])\n"
     ]
    }
   ],
   "source": [
    "#Q19-\n",
    "\n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['activation_date'] = pd.to_datetime(df['activation_date'])\n",
    "\n",
    "# Count the number of properties activated on each date\n",
    "activation_count = df['activation_date'].dt.date.value_counts()\n",
    "\n",
    "# Get the date(s) (if multiple maximum number of activations\n",
    "most_activated_dates = activation_count[activation_count == activation_count.max()]\n",
    "\n",
    "# Print\n",
    "print(f\"The most properties were activated on the following date(s): {most_activated_dates.index.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed768ef4-5b40-4635-bc2c-9a4415927368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of properties available for lease under the 'Anyone' category is: 40.94%\n"
     ]
    }
   ],
   "source": [
    "#Q20 - \n",
    "file_path = r\"C:\\\\Users\\\\Samyak\\\\Downloads\\\\64CSVfiles.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Count total number \n",
    "total_properties = len(df)\n",
    "\n",
    "# Count - ANYONE is all CAPS in dataset\n",
    "anyone_properties = len(df[df['lease_type'] == 'ANYONE'])\n",
    "\n",
    "percentage_anyone = (anyone_properties / total_properties) * 100\n",
    "\n",
    "print(f\"The percentage of properties available for lease under the 'Anyone' category is: {percentage_anyone:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbed6cf-cc0c-4ec8-b4a6-68d3424f8690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
